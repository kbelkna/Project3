---
title: "Project 3"
author: "Kara Belknap & Cassio Monti"
date: "2022-10-29"
output:
  github_document:
    html_preview: false
    toc: true
params:
  channel: "lifestyle"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, message = FALSE)
```

# Report for *`r params$channel`* Channel

This report contains Exploratory Data Analysis (EDA) about this data channel and a modeling section applying three regression methods. 

## Introduction

The objective of this analysis is to provide a comprehensive overview about publication metrics and their relationship with the number of shares that those publications presented during the study period. These data have been collected from Mashable website, one of the largest news websites from which the content of all the `r params$channel` channel articles published in 2013 and 2014 was extracted. These data were originally collected analyzed By Fernandes et al. (2015) work, in which the authors performed classification task comparing several machine learning algorithms. In the present study, the subset of the data used by Fernandes et al.(2015) corresponding to the data channel `r params$channel` is used for regression purposes. The response variable is the number of `shares` that the papers presented after publication. In other words, we will try to predict the number of shares the papers will have before publication. To perform the regression, Random Forest, Boosting, and Multiple Linear Regression are used. More information about the methods will be provided in further sections.    
   
   Some metrics have been calculated based on the information obtained from Marshable website. For instance, the Latent Dirichlet Allocation (LDA) was applied to the data set to identify the 5 top relevant topics and then measure the closeness of the current article to such topic. There are 5 relevance of topic metrics according to LDA:   
   
   - `LDA_00`: Closeness to LDA topic 0   
   - `LDA_01`: Closeness to LDA topic 1   
   - `LDA_02`: Closeness to LDA topic 2   
   - `LDA_03`: Closeness to LDA topic 3   
   - `LDA_04`: Closeness to LDA topic 4   

   Additionally, some quality metrics related to the keywords have been calculated and will be used in this analysis. These metrics represent the average number of shares for publications with worst, best, and average keywords. The classification of keywords under these groups was made by the authors of the original paper. The keyword metrics are shown below.   
   
   - `kw_avg_min`: Worst keyword (avg. shares)   
   - `kw_avg_max`: Best keyword (avg. shares)   
   - `kw_avg_avg`: Avg. keyword (avg. shares)   

   Article content metrics were also used in this study. These are general metrics about the body of the publication that can influence the number of shares of that paper. The content summary metrics are shown below.   
   
   -`num_videos`: Number of videos   
   -`n_tokens_content`: Number of words in the content   
   -`n_non_stop_unique_tokens`: Rate of unique non-stop words in the content   
   -`num_hrefs`: Number of links   
   -`num_self_hrefs`: Number of links to other articles published by Mashable   
   -`average_token_length`: Average length of the words in the content   

  These data were collected during 2013 and 2014 on daily basis. To represent time dependent information, a binary variable indicating whether the publication was made in a weekend or weekday, `is_weekend` is used.

## Required Packages

Before we can begin our analysis, we must load in the following packages:

```{r packages}

library(tidyverse)
library(caret)
library(knitr)
library(corrplot)

```

`Tidyverse` is used for data management and plotting through dplyr and ggplot packages. `Caret` package is used for data splitting and modeling. `Knitr` package is used for nice printing of tables. `Corrplot` is used for nice correlation plots assisting in visualization.

## Read in the Data

Using the data file `OnlineNewsPopularity.csv`, we will read in the data and add a new column corresponding to the type of data channel from which the data was classified. The new variable will be called `dataChannel`. Note that there are some rows that are unclassified according to the six channels of interest and those are indicated by `other`.   
  
  Once the data column is created, we can easily subset the data using the `filter` function to create a new data set for each data channel. We removed the original `data_channel_is_*` columns as well as two non-predictive columns `url` and `timedelta`.

```{r dataImport}

# reading in the data set
rawData <- read_csv("../OnlineNewsPopularity.csv")

# creating new variable to have more comprehensive names for data channels.
rawDataChannel <- rawData %>%
  mutate(dataChannel = ifelse(data_channel_is_lifestyle == 1, "lifestyle", 
                              ifelse(data_channel_is_entertainment == 1, "entertainment", 
                              ifelse(data_channel_is_bus == 1, "bus", 
                              ifelse(data_channel_is_socmed == 1, "socmed", 
                              ifelse(data_channel_is_tech == 1, "tech", 
                              ifelse(data_channel_is_world == 1, "world", 
                                     "other"))))))) %>%
  select(-data_channel_is_lifestyle, -data_channel_is_entertainment, 
         -data_channel_is_bus, -data_channel_is_socmed, -data_channel_is_tech,
         -data_channel_is_world, -url, -timedelta)

# assigning channel data to R objects.
lifestyleData <- rawDataChannel %>%
  filter(dataChannel == "lifestyle")

entertainmentData <- rawDataChannel %>%
  filter(dataChannel == "entertainment")

busData <- rawDataChannel %>%
  filter(dataChannel == "bus")

socmedData <- rawDataChannel %>%
  filter(dataChannel == "socmed")

techData <- rawDataChannel %>%
  filter(dataChannel == "tech")

worldData <- rawDataChannel %>%
  filter(dataChannel == "world")

```


## Select Data for Appropriate Data Channel

To select the appropriate data channel based on the `params$channel`, we created a function `selectData` which would return the appropriate data set and assign it to the data set `activeData`. This will be the file we will use for the remainder of the report.   
  
  To select the appropriate data channel based on the `params$channel`, we created a function `selectData` which would return the appropriate data set and assign it to the data set `activeData`. This will be the file we will use for the remainder of the report.

```{r selectData}

# function to assign automated calls for the different data channels
selectData <- function(dataChannel) { 
  if (dataChannel == "lifestyle"){
    return(lifestyleData)
  }
  if (dataChannel == "entertainment"){
    return(entertainmentData)
  }
  if (dataChannel == "bus"){
    return(busData)
  }
  if (dataChannel == "socmed"){
    return(socmedData)
  }
  if (dataChannel == "tech"){
    return(techData)
  }
  if (dataChannel == "world"){
    return(worldData)
  }
}

# activating corresponding data set.
dataChannelSelect <- params$channel

activeData <- selectData(dataChannelSelect)

```


## Summarizations for data channel *`r params$channel`*

In this section, we will perform EDA for the data channel `r params$channel`

### Data Split

This section splits the data set into training and test sets for the proportion of 80/20. The data summarization will be conducted on the training set. To split the data, the function `createDataPartition()`, from `caret` package, was used with the argument `p=0.8` to represent 80% of the data should be in the split. The function `set.seed(555)` was used to fix the random seed. The code below shows the creation of training and test sets.

```{r splitData}

set.seed(555)

trainIndex <- createDataPartition(activeData$shares, p = 0.8, list = FALSE)

activeTrain <- activeData[trainIndex, ]

activeTest <- activeData[-trainIndex, ]

```


### Data manipulation for statistics

A new object is created in this section aiming to summarize publications during weekdays and weekends and create factor levels for them to match with `shares` variable. The functions `ifelse()` was used to vectorize the IF-ELSE statements associated to `mutate()` which took care of attaching the new variable to the data set. The function `factor()` was used to explicitly coerce the days of week into levels of the newly created categorical variable "Day".

```{r statsData}

# IF-ELSE statements
statsData <- activeTrain %>%
  mutate(Day = ifelse(weekday_is_monday == 1, "Monday", 
                      ifelse(weekday_is_tuesday == 1, "Tuesday", 
                      ifelse(weekday_is_wednesday == 1, "Wednesday", 
                      ifelse(weekday_is_thursday == 1, "Thursday", 
                      ifelse(weekday_is_friday == 1, "Friday", 
                      ifelse(weekday_is_saturday == 1, "Saturday", 
                      ifelse(weekday_is_sunday == 1, "Sunday",
                             "missingdata")))))))) %>%
  mutate(Weekend = ifelse(is_weekend == 1, "Yes", "No"))

# Assigning factor levels
statsData$Day <- factor(statsData$Day, 
                levels = c("Monday", "Tuesday", "Wednesday", "Thursday", 
                           "Friday", "Saturday", "Sunday"))

```

### Belknap - Summary Statistics

The following table gives us information about the summary statistics for the number of shares for articles in the data channel `r params$channel`. The `summary()` function was used to extract these metrics.

```{r summaryStats}

summary(activeTrain$shares)

```

The following table gives us information about the average, median, and standard deviation for the number of shares based on whether the post was made on a weekend or a weekday. The variable "weekend" was grouped, via `grouped_by()`, and for each level the sum, average, median, and standard deviation of shares were calculated via `sum()`, `mean()`, `meadian()`, `sd()`, and `summarise()` functions. The summary table is shown below.

```{r table1}

statsData %>% 
  group_by(Weekend) %>%
  summarise(sumShares = sum(shares), avgShares = mean(shares), medShares = median(shares), sdShares = sd(shares))

```

Likewise, this table gives us information about the number of shares by the day of the week. The same functions were used here, by applied to levels of variable "Day". Also, the quantities maximum `max()` and minimum `min()` number of shares by levels of "Day" were calculated.

```{r table2}

statsData %>% 
  group_by(Day) %>%
  arrange(Day) %>%
  summarise(sumShares = sum(shares), avgShares = mean(shares), medShares = median(shares), sdShares = sd(shares), maxShares = max(shares),
            minShares = min(shares))

```


### Monti - Summary Statistics

```{r}

# two-way contingency table between shares and Days



```


```{r}

# correlation matrix



```


### Monti - Graphs (3)

```{r}

# bivariate correlation plot



```


```{r}

# relationship between shares and LDA levels (facet_wrap+smooth)



```

```{r}

# relationship between shares and keyword metrics (color)



```

```{r}

# relationship between shares and content metrics (facet_wrap+smooth)



```


### Belknap - Graphs (3)

The following graph shows the number of shares compared to the number of words in the title. The output is colored by the day of the week. 

```{r titlewordcountGraph}

titlewordcountGraph <- ggplot(statsData, aes(x = n_tokens_title, y = shares))
titlewordcountGraph + geom_point(aes(color = Day)) + 
  ggtitle("Number of Shares vs. Number of Words in Title") +
  ylab("Number of Shares") +
  xlab("Number of Words in Title")

```

The following plot shows the number of shares by the rate of positive words in the article. A positive trend would indicate that articles with more positive words are shared more often than articles with negative words.

```{r positivewordrateGraph}

positivewordrateGraph <- ggplot(statsData, aes(x = rate_positive_words, y = shares))
positivewordrateGraph + geom_point(aes(color = Day)) + 
  ggtitle("Number of Shares vs. Rate of Positive Words") +
  ylab("Number of Shares") +
  xlab("Rate of Positive Words") 

```

The following plot shows the total number of shares as related to the parameter title subjectivity. A positive trend would indicate that articles are shared more often when the title is subjective. A negative trend would indicate that articles are shared more often when the title is less subjective.

```{r titleSubjectivityGraph, eval=FALSE}

titleSubjectivityGraph <- ggplot(statsData, aes(x = title_subjectivity, y = shares))
titleSubjectivityGraph + geom_point(aes(color = n_tokens_title)) + 
  ggtitle("Number of Shares vs. Title Subjectivity") +
  ylab("Number of Shares") +
  xlab("Title Subjectivity") + 
  labs(color = "Word Count in Title")

```

### Subsetting Variables for Modeling

```{r choseVars}

dfTrain = activeTrain %>%
  select(shares, starts_with("LDA_"), average_token_length,
         is_weekend, n_tokens_content, n_non_stop_unique_tokens, num_hrefs,
         num_self_hrefs, num_videos, average_token_length, kw_avg_min, 
         kw_avg_max, kw_avg_avg, is_weekend)

dfTest = activeTest %>%
  select(shares, starts_with("LDA_"), average_token_length,
         is_weekend, n_tokens_content, n_non_stop_unique_tokens, num_hrefs,
         num_self_hrefs, num_videos, average_token_length, kw_avg_min, 
         kw_avg_max, kw_avg_avg, is_weekend)

# looking for NAs
anyNA(dfTrain)
anyNA(dfTest)

```





## Modeling

In this section, we will perform regression for prediction purposes for the data channel `r params$channel`.

### Belknap - Linear Regression Model Explanation

### Monti - Linear Regression Model

```{r}

LASSO = train(shares~., data = dfTrain,
              method="glmnet",
              preProcess = c("center","scale"),
              tuneGrid = expand.grid(alpha = 1, lambda = seq(0,10,0.1)),
              trControl = trainControl(method="CV",number=5))

plot(LASSO)

coef(LASSO$finalModel, LASSO$bestTune$lambda)

```


```{r}
postResample(pred = predict(LASSO, newdata = dfTest), obs = dfTest$shares)

```



### Belknap - Linear Regression Model

### Monti - Ensemble Tree-based Model

### Belknap - Ensemble Tree-based Model

### Belknap - Random Forest Model & Explanation

```{r RF}

train.control = trainControl(method = "cv", number = 5)

rfFit <- train(shares~.,
               data = dfTrain,
               method = "rf",
               trControl = train.control,
               preProcess = c("center","scale"),
               tuneGrid = data.frame(mtry = 1:5))

rfFit$bestTune$mtry

```



```{r}

plot(rfFit)

```


```{r}

rfFit$results

```


```{r}

RF_pred <- predict(rfFit, newdata = activeTest)

metric_rf = postResample(RF_pred, activeTest$shares)

metric_rf

```


### Monti - Boosted Tree Model & Explanation

```{r Boosting}

tunG = expand.grid(n.trees = seq(25,200,25),
                      interaction.depth = 1:4,
                      shrinkage = 0.1,
                      n.minobsinnode = 10)

gbmFit <- train(shares~.,
               data = dfTrain,
               method = "gbm",
               preProcess = c("center","scale"),
               trControl = train.control,
               tuneGrid = tunG,
               verbose = FALSE
               )


gbmFit$bestTune$n.trees

gbmFit$bestTune$interaction.depth

```

```{r}

plot(gbmFit)

```

```{r}
gbmFit$results

```


```{r}
gbm_pred <- predict(gbmFit, newdata = activeTest)

metric_boosting = postResample(gbm_pred, activeTest$shares)

metric_boosting

```


## Comparison & Conclusion - Monti

```{r comparison}

bestMethod = function(x){
  
  bestm = which.min(lapply(1:length(x), function(i) x[[i]][1]))
  
  out = switch(bestm,
                "Random Forest",
                "Boosting")
  
  return(out)
  
}

tb = data.frame(RF = metric_rf, Boosting = metric_boosting)

kable(tb, caption = "Accuracy Metric by Ensemble Method on Test Set",
      digits = 3)

```

The best model is `r bestMethod(tb)` based on RMSE metric.


