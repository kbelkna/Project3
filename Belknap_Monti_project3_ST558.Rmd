---
title: "Project 3"
author: "Kara Belknap & Cassio Monti"
date: "2022-11-5"
output:
  github_document:
    html_preview: false
    toc: true
params:
  channel: "lifestyle"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, message = FALSE)
```

# Report for *`r params$channel`* Channel

This report contains Exploratory Data Analysis (EDA) about the `r params$channel` data channel and a modeling section applying three regression methods which attempt to predict trends about article sharing on the Mashable website. 

## Introduction

The objective of this analysis is to provide a comprehensive overview about publication metrics and their relationship with the number of shares that those publications presented during the study period. These data have been collected from Mashable website, one of the largest news websites from which the content of all the `r params$channel` channel articles published in 2013 and 2014 was extracted. These data were originally collected analyzed by Fernandes et al. (2015), in which the authors performed classification task comparing several machine learning algorithms. In the present study, the subset of the data used by Fernandes et al.(2015) corresponding to the data channel `r params$channel` is used for regression purposes. The response variable is the number of `shares` that the papers presented after publication. In other words, we will try to predict the number of shares the papers will have before publication. To perform the regression, Random Forest, Boosting, and Multiple Linear Regression are used. More information about the methods will be provided in further sections.    
   
   Some metrics have been calculated based on the information obtained from Mashable website. For instance, the Latent Dirichlet Allocation (LDA) was applied to the data set to identify the 5 top relevant topics and then measure the closeness of the current article to such topic. There are 5 relevance of topic metrics according to LDA:   
   
   - `LDA_00`: Closeness to LDA topic 0   
   - `LDA_01`: Closeness to LDA topic 1   
   - `LDA_02`: Closeness to LDA topic 2   
   - `LDA_03`: Closeness to LDA topic 3   
   - `LDA_04`: Closeness to LDA topic 4   

   Additionally, some quality metrics related to the keywords have been calculated and will be used in this analysis. These metrics represent the average number of shares for publications with worst, best, and average keywords. The classification of keywords under these groups was made by the authors of the original paper. The keyword metrics are shown below.   
   
   - `kw_avg_min`: Worst keyword (avg. shares)   
   - `kw_avg_max`: Best keyword (avg. shares)   
   - `kw_avg_avg`: Avg. keyword (avg. shares)   

   Article content metrics were also used in this study. These are general metrics about the body of the publication that can influence the number of shares of that paper. The content summary metrics are shown below.   
   
   - `num_videos`: Number of videos   
   - `n_tokens_content`: Number of words in the content   
   - `n_non_stop_unique_tokens`: Rate of unique non-stop words in the content   
   - `num_hrefs`: Number of links   
   - `num_self_hrefs`: Number of links to other articles published by Mashable   
   - `average_token_length`: Average length of the words in the content   

  These data were collected during 2013 and 2014 on daily basis. To represent time dependent information, a binary variable indicating whether the publication was made in a weekend or weekday, `is_weekend` is used.
  
  The original data set, along with information about all of the variables, can be found [here](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity).

## Data Import and Manipulation

Prior to beginning our analysis, we must import the data and subset it to only look at our data channel of interest.

### Required Packages

Before we can begin our analysis, we must load in the following packages:

```{r packages}

library(tidyverse)
library(caret)
library(PerformanceAnalytics)
library(knitr)

```

`Tidyverse` is used for data management and plotting through dplyr and ggplot packages. `Caret` package is used for data splitting and modeling. `PerformanceAnalytics` is used for nice correlation plots assisting in the visualization. `knitr` package is used to provide nice looking tables. 

### Read in the Data

Using the data file `OnlineNewsPopularity.csv`, we will read in the data and add a new column corresponding to the type of data channel from which the data was classified. The new variable will be called `dataChannel`. Note that there are some rows that are unclassified according to the six channels of interest and those are indicated by `other`. The data indicated by `other` was excluded from all reports since the data had not been assigned to one of our channels of interest. 
  
  Once the data column is created, we can easily subset the data using the `filter` function to create a new data set for each data channel. We removed the original `data_channel_is_*` columns as well as two non-predictive columns `url` and `timedelta`.

```{r dataImport}

# reading in the data set
rawData <- read_csv("../OnlineNewsPopularity.csv")

# creating new variable to have more comprehensive names for data channels.
rawDataChannel <- rawData %>%
  mutate(dataChannel = ifelse(data_channel_is_lifestyle == 1, "lifestyle", 
                              ifelse(data_channel_is_entertainment == 1, "entertainment", 
                              ifelse(data_channel_is_bus == 1, "bus", 
                              ifelse(data_channel_is_socmed == 1, "socmed", 
                              ifelse(data_channel_is_tech == 1, "tech", 
                              ifelse(data_channel_is_world == 1, "world", 
                                     "other"))))))) %>%
  select(-data_channel_is_lifestyle, -data_channel_is_entertainment, 
         -data_channel_is_bus, -data_channel_is_socmed, -data_channel_is_tech,
         -data_channel_is_world, -url, -timedelta)

# assigning channel data to R objects.
lifestyleData <- rawDataChannel %>%
  filter(dataChannel == "lifestyle")

entertainmentData <- rawDataChannel %>%
  filter(dataChannel == "entertainment")

busData <- rawDataChannel %>%
  filter(dataChannel == "bus")

socmedData <- rawDataChannel %>%
  filter(dataChannel == "socmed")

techData <- rawDataChannel %>%
  filter(dataChannel == "tech")

worldData <- rawDataChannel %>%
  filter(dataChannel == "world")

```


### Select Data for Appropriate Data Channel

To select the appropriate data channel based on the `params$channel`, we created a function `selectData` which would return the appropriate data set and assign it to the data set `activeData`. This will be the file we will use for the remainder of the report. 

```{r selectData}

# function to assign automated calls for the different data channels
selectData <- function(dataChannel) { 
  if (dataChannel == "lifestyle"){
    return(lifestyleData)
  }
  if (dataChannel == "entertainment"){
    return(entertainmentData)
  }
  if (dataChannel == "bus"){
    return(busData)
  }
  if (dataChannel == "socmed"){
    return(socmedData)
  }
  if (dataChannel == "tech"){
    return(techData)
  }
  if (dataChannel == "world"){
    return(worldData)
  }
}

# activating corresponding data set.
dataChannelSelect <- params$channel

activeData <- selectData(dataChannelSelect)

```

## Summarizations for the *`r params$channel`* Data Channel 

In this section, we will perform EDA for the data channel `r params$channel`

### Data Manipulation for EDA

A new object is created in this section aiming to summarize publications during weekdays and weekends and create factor levels for them to match with `shares` variable. The functions `ifelse()` was used to vectorize the IF-ELSE statements associated to `mutate()` which took care of attaching the new variable to the data set. The function `factor()` was used to explicitly coerce the days of week into levels of the newly created categorical variable "Day".

```{r statsData}

# IF-ELSE statements
statsData <- activeData %>%
  mutate(Day = ifelse(weekday_is_monday == 1, "Monday", 
                      ifelse(weekday_is_tuesday == 1, "Tuesday", 
                      ifelse(weekday_is_wednesday == 1, "Wednesday", 
                      ifelse(weekday_is_thursday == 1, "Thursday", 
                      ifelse(weekday_is_friday == 1, "Friday", 
                      ifelse(weekday_is_saturday == 1, "Saturday", 
                      ifelse(weekday_is_sunday == 1, "Sunday",
                             "missingdata")))))))) %>%
  mutate(Weekend = ifelse(is_weekend == 1, "Yes", "No"))

# Assigning factor levels
statsData$Day <- factor(statsData$Day, 
                levels = c("Monday", "Tuesday", "Wednesday", "Thursday", 
                           "Friday", "Saturday", "Sunday"))

```

###  EDA: Summary Statistics

#### Summary Statistics, Number of Articles Shared

The following table gives us information about the summary statistics for the number of shares for articles in the data channel `r params$channel`. The `summary()` function was used to extract these metrics.

```{r summaryStats}

summary(statsData$shares)

```

#### Summary Statistics, Number of Articles Shared, Weekend vs. Weekday

The following table gives us information about the average, median, and standard deviation for the number of shares based on whether the post was made on a weekend or a weekday. The variable "weekend" was grouped, via `grouped_by()`, and for each level the sum, average, median, and standard deviation of shares were calculated via `sum()`, `mean()`, `meadian()`, `sd()`, and `summarise()` functions. The summary table is shown below.

```{r table1}

statsData %>% 
  group_by(Weekend) %>%
  summarise(sumShares = sum(shares), avgShares = mean(shares), medShares = median(shares), sdShares = sd(shares))

```

#### Summary Statistics, Articles Shared by Day of Week

Likewise, this table gives us information about the number of shares by the day of the week. The same functions were used here, by applied to levels of variable "Day". Also, the quantities maximum `max()` and minimum `min()` number of shares by levels of "Day" were calculated.

```{r table2}

statsData %>% 
  group_by(Day) %>%
  arrange(Day) %>%
  summarise(sumShares = sum(shares), avgShares = mean(shares), medShares = median(shares), sdShares = sd(shares), maxShares = max(shares),
            minShares = min(shares))

```

#### Total Articles Shared by Day of Week

Next, we will analyse the frequency of occurrence of publications on each day of the week. The one-way contingency table below presents those frequencies.

```{r}

table(statsData$Day)

```

#### Contingency Table

Another discrete analysis performed here is the two-way contingency table related to the discretization of the response variable if we divided `shares` into two categories. The function `cut()` was used for the end. In this case, we count the frequency of the number of publications in days of week with the two levels of response variable. These levels represent smaller number of shares (on the left) and larger number of shares (on the right). The table below shows this counting.

```{r}

table(statsData$Day, cut(statsData$shares, breaks = 2))


```

#### Correlation Matrix

An important EDA analysis for regression tasks is the calculation of correlation matrix. The function `cor()` is used in this section to return the top 10 most correlated potential predictor variables with the response variable `shares`. The code below presents the process of obtaining these variables and their respective correlations with the response variable. 

```{r correl}

var_sel = select(statsData,starts_with("LDA_"), average_token_length,
         is_weekend, n_tokens_content, n_non_stop_unique_tokens, num_hrefs,
         num_self_hrefs, num_videos, average_token_length, kw_avg_min, 
         kw_avg_max, kw_avg_avg, is_weekend)

# correlation matrix
correlation = cor(statsData$shares, var_sel)

# sorting the highest correlations
p = sort(abs(correlation), decreasing = T)

# getting column ID
var_id = which(abs(correlation) %in% p[1:10])

# collecting variable names
var_cor = colnames(correlation)[var_id]

#combining names with correlations
tbcor = cbind(var_cor, correlation[var_id])

# converting to tibble
tbcor = as_tibble(tbcor)

# updating column names
colnames(tbcor)=c("Variables","Correlation")

# rounding the digits
tbcor$Correlation = round(as.numeric(tbcor$Correlation),3)

# nice printing with kable
kable(tbcor, caption = "Top 10 Response Correlated Variables")

```

The variables that present most correlation with the response variable `shares` are `r var_cor`. These variables will be studied in more depth via PCA to understand the orientation of the most variable potential predictors. 

#### PCA

The code below presents the PCA analysis as part of the EDA.

```{r PCA}

id = which(colnames(statsData) %in% var_cor)

# PCA
PC = prcomp(statsData[,id], center = TRUE, scale = TRUE)

pc_directions=as.data.frame(PC$rotation)

kable(pc_directions, caption="PCs for EDA", digits = 3)

```

### EDA: Graphical Analysis

#### Correlation Plot

The plot below presents histograms, scatter plots, and correlations in a bivariate structure of the top 5 variables chosen the correlation analysis. Notice the shape of the distributions and the values of the correlations for the first column, which the one related to the response variable `shares`.

```{r correlplot}

# bivariate correlation plot
cor_data <- select(statsData,shares,var_id[1:5])
chart.Correlation(cor_data, histogram=TRUE)

```

#### PCA: Biplot

The biplot below presents the PC1 and PC2 from the PCA analysis. The function `ggplot()` was used to create the plot and the segments created via `geom_segment()` were rescaled so that we could better see the variable names. The most variation in the data is contained in the PC1, hence, the most important variables in the data approximately are oriented towards the axis of PC1 and, therefore, may be good predictors for the `shares` response. Likewise, for PC2, which contains the second most variability in the data set, the variables that are oriented approximately towards the axis of PC2 are the second most important variables.

```{r biplot}

pc_df<-data.frame(PC$x)
# plotting PC1 and PC2 for the top 5 variables
# biplot(PC, cex = 1)
ggplot(pc_directions)+
  geom_point(data = pc_df, mapping = aes(x=PC1, y=PC2))+
  geom_segment(aes(x = 0, y = 0, yend = 50 * PC2, xend = 50 * PC1))+
  geom_label(mapping = aes(x = 51 * PC1, y = 51 * PC2, label = row.names(pc_directions)))

```

#### Scatter Plots by LDA Value


The scatter plots below show the different levels of the variables related to the LDA metrics, from 0 to 4, and graphs the relationship with the response variable `shares`. The function `ggplot()` is used to create the plot frame and `geom_point()`, `geom_smooth`, and `facert_wrap()` function are used to plot the scatter points, the smooth GAM (Generalized Additive Models) lines, and split the data by LDA type, respectively. It is possible to see the behavior of the response variable in relation to each LDA types.

```{r LDAplot}

LDA.dat = statsData %>%
  select(shares, starts_with("LDA")) %>%
  pivot_longer(cols = LDA_00:LDA_04, names_to = "LDA", values_to = "values")

# relationship between shares and LDA levels (facet_wrap+smooth)
ggplot(LDA.dat, aes(y = shares, x = values))+
  geom_point() + geom_smooth(method = "loess")+ facet_wrap(~LDA)+
labs(x = "LDA Values", y = "Shares", title = "Shares by LDA Types")


```

#### Scatter Plots by Keyword Metrics

The scatter plots below show the different types of the variables related to the keyword metrics and graphs the relationship with the response variable `shares`. The function `ggplot()` is used to create the plot frame and `geom_point()`, `geom_smooth`, and `facert_wrap()` function are used to plot the scatter points, the smooth GAM (Generalized Additive Models) lines, and split the data by keyword type, respectively. It is possible to see the behavior of the response variable in relation to each of the 3 keyword metric types.

```{r keywordplot}

# relationship between shares and keyword metrics
kw.dat = statsData %>%
  select(shares, kw_avg_max, kw_avg_avg, kw_avg_min) %>%
  pivot_longer(cols = 2:4, names_to = "keyword", values_to = "values")

# relationship between shares and keyword metrics types (facet_wrap+smooth)
ggplot(kw.dat, aes(y = shares, x = values))+
  geom_point() + geom_smooth(method = "loess")+ facet_wrap(~keyword)+
labs(x = "Keyword Metric Values", y = "Shares", title = "Shares by Keyword Metric Types")

```

#### Scatter Plots by Content Metrics

Finally, the scatter plots below show the different types of the variables related to the Content metrics and graphs the relationship with the response variable `shares`. The function `ggplot()` is used to create the plot frame and `geom_point()`, `geom_smooth`, and `facet_wrap()` function are used to plot the scatter points, the smooth GAM (Generalized Additive Models) lines, and split the data by content type, respectively. It is possible to see the behavior of the response variable in relation to each of the 4 content metric types.

```{r Contentplot}

# relationship between shares and content metrics (facet_wrap+smooth)
cont.dat = statsData %>%
  select(shares, num_videos, n_tokens_content, n_non_stop_unique_tokens,
         num_hrefs, num_self_hrefs, average_token_length) %>%
  pivot_longer(cols = 2:7, names_to = "content", values_to = "values")

# relationship between shares and content metrics types (facet_wrap+smooth)
ggplot(cont.dat, aes(y = shares, x = values))+
  geom_point() + geom_smooth(method = "loess")+ facet_wrap(~content)+
labs(x = "Content Metric Values", y = "Shares", title = "Shares by Content Metric Types")

```

#### Scatter Plot of Title Words 

The following graph shows the number of shares compared to the number of words in the title. The output is colored by the day of the week. 

```{r titlewordcountGraph}

titlewordcountGraph <- ggplot(statsData, aes(x = n_tokens_title, y = shares))
titlewordcountGraph + geom_point(aes(color = Day)) + 
  ggtitle("Number of Shares vs. Number of Words in Title") +
  ylab("Number of Shares") +
  xlab("Number of Words in Title")

```

#### Scatter Plot of Positive Words

The following plot shows the number of shares by the rate of positive words in the article. A positive trend would indicate that articles with more positive words are shared more often than articles with negative words. The output is again colored by the day of the week.

```{r positivewordrateGraph}

positivewordrateGraph <- ggplot(statsData, aes(x = rate_positive_words, y = shares))
positivewordrateGraph + geom_point(aes(color = Day)) + 
  ggtitle("Number of Shares vs. Rate of Positive Words") +
  ylab("Number of Shares") +
  xlab("Rate of Positive Words") 

```

#### Scatter Plot of Title Subjectivity

The following plot shows the total number of shares as related to the parameter title subjectivity. A positive trend would indicate that articles are shared more often when the title is subjective. A negative trend would indicate that articles are shared more often when the title is less subjective.

```{r titleSubjectivityGraph}

titleSubjectivityGraph <- ggplot(statsData, aes(x = title_subjectivity, y = shares))
titleSubjectivityGraph + geom_point(aes(color = n_tokens_title)) + 
  ggtitle("Number of Shares vs. Title Subjectivity") +
  ylab("Number of Shares") +
  xlab("Title Subjectivity") + 
  labs(color = "Word Count in Title")

```

## Modeling

In this section, we will perform regression for prediction purposes for the data channel `r params$channel`. All models were fitted using 5-fold Cross-Validation via `train()` function from `caret` package. We will model the data using Linear Regression, Random Forest, and Boosted Tree modeling methods.

### Data Manipulation for Modeling

#### Data Split

This section splits the data set into training and test sets for the proportion of 70/30. All modeling will be conducted on the training set. To split the data, the function `createDataPartition()`, from `caret` package, was used with the argument `p=0.7` to represent 70% of the data should be in the split. The function `set.seed(555)` was used to fix the random seed. The code below shows the creation of training and test sets.

```{r splitData}

set.seed(555)

trainIndex <- createDataPartition(activeData$shares, p = 0.7, list = FALSE)

activeTrain <- activeData[trainIndex, ]

activeTest <- activeData[-trainIndex, ]

```

#### Subsetting Variables for Modeling

The variables selected below are those described in the introduction of this study and will be used in the modeling section. The function `select()` was used to subset the corresponding variables from the training and test sets and two new objects are created specially for the modeling section, `dfTrain` and `dfTest`.

```{r choseVars}

dfTrain = activeTrain %>%
  select(shares, starts_with("LDA_"), average_token_length,
         is_weekend, n_tokens_content, n_non_stop_unique_tokens, num_hrefs,
         num_self_hrefs, num_videos, average_token_length, kw_avg_min, 
         kw_avg_max, kw_avg_avg, is_weekend)

dfTest = activeTest %>%
  select(shares, starts_with("LDA_"), average_token_length,
         is_weekend, n_tokens_content, n_non_stop_unique_tokens, num_hrefs,
         num_self_hrefs, num_videos, average_token_length, kw_avg_min, 
         kw_avg_max, kw_avg_avg, is_weekend)

```

### Linear Regression Modeling

Linear regression is a modeling technique by which one attempts to model a response variable (in this case `shares`) with one or more explanatory variables using a straight line. If there is only one explanatory variable, you would call this simple linear regression. Using more than one explanatory variable is called multiple linear regression. For the purposes of this report, we will be looking at multiple linear regression (MLR).

The basic formula that is used for multiple linear regression is 

$$ 
Y_i = \beta_0 + \beta_{1}x_{1i} + \beta_{2}x_{2i} + ... + \beta_{n}x_{ni} + E_i
$$
Here:   
   - $Yi$ is our response variable for the $i^{th}$ observation   
   - $x_{i}$ is the value of our explanatory variable for the $i^{th} observation for each explanatory variable (1-n)   
   - $\beta_0$ is the y intercept   
   - $\beta_{1,...,n)$ is the regression coefficient corresponding to the explanatory variable of interest (1-n)   
   - $E_i$ is an error parameter   

The goal of linear regression is to model the fit by minimizing the sum of squared errors. 

$$
SSE = \sum_{i=1}^n(y_{i} - \hat{y}_{i})^2
$$
Here:   
   - $y_i$ is the actual value   
   - $\hat{y}_{i}$ is the predicted value   

In R, MLR is generally done with the function `lm`. There are also a variety of other methods that fall under the umbrella of MLR, namely LASSO regression which we will explore as part of this analysis.

#### Linear Regression Model #1: Multiple Linear Regression Using `lm`

Here, modeling for linear regression is done with the `caret` package using the method `lm`. The `summary` function gives us the regression coefficients.

```{r linearTrain}

lmFit = train(shares~., data = dfTrain,
              method="lm",
              preProcess = c("center","scale"),
              trControl = trainControl(method="CV",number=5))

summary(lmFit)

```

The following table shows the output training metrics for this linear regression. 

```{r linearTrainResult}

lm_out = data.frame(lmFit$results)

kable(lm_out, caption = "Output Training Metrics for Linear Regression",
      digits = 3)

```

The following shows the RMSE, $R^2$, and MAE values for the model as it performed on predicting the test set.

```{r lineartest}

metric_lm = postResample(pred = predict(lmFit, newdata = dfTest), 
                         obs = dfTest$shares)

metric_lm

```

#### Linear Regression Model #2: LASSO Regression using `glmnet`

The linear regression chosen for this next model is based on penalized regression via LASSO method. This method has a particular advantage of having a triangular shape of parameters search space so that it allows the estimated coefficients to be zero. Hence, LASSO regression is also a variable selection method. In this application, we will test the prediction capability of LASSO regression only. It was tested a sequence of values for the Regularization Parameter ($\lambda$), a tuning parameter, from 0 to 10 by 1 via `seq(0,10,1)` assigned to the `tuneGrid = `argument in the `train()` function from `caret` package. The code below presents the estimated coefficients for the best hyperparameter.

```{r LASSOtrain}

LASSO = train(shares~., data = dfTrain,
              method="glmnet",
              preProcess = c("center","scale"),
              tuneGrid = expand.grid(alpha = 1, lambda = seq(0,10,1)),
              trControl = trainControl(method="CV",number=5))

coef(LASSO$finalModel, LASSO$bestTune$lambda)

```

The best $\lambda$ for this model is `r LASSO$bestTune$lambda` and this value can be seen in the table below which summarizes all the metrics for the 5-fold cross-validation.

```{r LASSOresultTrain}

lasso_out = data.frame(LASSO$results)

kable(lasso_out, caption = "Output Training Metrics for LASSO",
      digits = 3)

```

The plot below shows the RMSE by Regularization Parameter ($\lambda$). It is easy to see that RMSE is minimized when $\lambda$ = `r LASSO$bestTune$lambda`.

```{r LASSOplot}

plot(LASSO)

```

The validation step for LASSO regression is applied on the test set after predicting the response variable for unseen data (test set). By using `predict()` and `postResample()` functions, the metrics RMSE (Root Means Squared Error), $R^2$ (Coefficient of Determination), and MAE (Mean Absolute Error) are calculated and displayed below.

```{r LASSOtest}

metric_LASSO = postResample(pred = predict(LASSO, newdata = dfTest),
                            obs = dfTest$shares)

metric_LASSO

```

### Tree-Based Modeling

The next two models, Random Forest and Boosted Tree, are both types of tree-based modeling methods. Generally speaking, in a tree-based modeling method, the predictor space is split into regions, with different predictions for each region. In the case of regression trees where the goal is to predict a continuous response, the mean of observations for a given region is typically used to make the predictions. 

To make the predictions, the trees are split using recursive binary splitting. For every possible value of each predictor, find the residual sum of squares (RSS) and try to minimize that. The process is repeated with each split. Often, trees are grown very large and need to be cut back using cost complexity pruning. This ensures that the model is not overfit and will work well on prediction of new data.

#### Random Forest Model

In this section, we attempt to model the data using a Random Forest model, which is a type of ensemble learning which averages multiple tree models in order to lower the variance of the final model and thus improve our prediction.

In a random forest model, we first begin by creating multiple trees from bootstrap samples. A random subset of predictors is used to create each bootstrap sample. The predictors are selected randomly to prevent the trees from being correlated. If the random subset was not used (as in another tree based method called bagging), the trees would likely all choose the same predictors for the first split. Choosing the splits randomly avoids this correlation. The number of predictors is specified by `mtry`. The maximum number of predictors for a regression model is generally chosen to be the total number of predictors divided by 3. Once the bootstrap sample statistics are collected, they are averaged and used to select a final model. 

Random forest models use "out of bag" error to test the data using samples from the original data set that were not included in a particular bootstrap data set. 

For the random forest model, we will use the `train` function from the `caret` package. We set the `mtry` to select 1-5 predictors. 


```{r RFtrain}

train.control = trainControl(method = "cv", number = 5)

rfFit <- train(shares~.,
               data = dfTrain,
               method = "rf",
               trControl = train.control,
               preProcess = c("center","scale"),
               tuneGrid = data.frame(mtry = 1:5))

rfFit$bestTune$mtry

```
The best `mtry` for this particular model was `r rfFit$bestTune$mtry`.

The following plot shows the RMSE values for each of the tune. The objective in random forest modeling is to choose the model with the lowest RMSE. 

```{r RFplot}

plot(rfFit)

```

The following table shows training metrics for the random forest model. Again, the best model is the one that minimizes RMSE.

```{r RFresults}

rf_out = data.frame(rfFit$results)

kable(rf_out, caption = "Output Training Metrics for Random Forest",
      digits = 3)

```

Now we will run a prediction on our test data split that we obtained when we split the data based on a 70/30 split. The table shows the RMSE value for the test data set, which is an indication of how well our model worked to predict data that was not included when training the original model. We can compare this model against other models to find the model with the lowest RMSE.

```{r RFTest}

RF_pred <- predict(rfFit, newdata = activeTest)

metric_rf = postResample(RF_pred, activeTest$shares)

metric_rf

```

#### Boosted Tree Model

Another type of ensemble learning with trees is the boosted tree model. In this model, all of the predictors are used, but the trees are grown sequentially instead of in parallel. Instead of using boostrap samples, each subsequent tree is grown based on the tree before it. 

When the first tree is grown, all predictions are initialized as zero. Once the first tree is fit, we use the residuals from that tree instead of the response variable to fit the next tree. The idea is to grow the tree very slowly (controlled by shrinkage parameter $\lambda$) which will allow the tree to slowly minimize the residuals to find the best model. 

Boosted tree models require three tuning parameters:   
   1. $\lambda$ is a shrinkage parameter that aims to slow the growth of the trees (indicated in the model by `shrinkage`). In this model, we will use $0.1$.  
   2. $B$ is the number of trees (indicated in the model by `n.trees`). In this model, we will use `seq(25,200,25)`.   
   3. $d$ is the number of splits (indicated in the model by `interaction.depth`) In this model, we will use values of 1-4.  

A fourth tuning parameter, `n.minobsinnode`, may also be specified. It is related to number of observations in each terminal node, which generally corresponds to the size of the tree. Here, the default of `10` is used.

```{r boostingTrain}

tunG = expand.grid(n.trees = seq(25,200,25),
                      interaction.depth = 1:4,
                      shrinkage = 0.1,
                      n.minobsinnode = 10)

gbmFit <- train(shares~.,
               data = dfTrain,
               method = "gbm",
               preProcess = c("center","scale"),
               trControl = train.control,
               tuneGrid = tunG,
               verbose = FALSE
               )


gbmFit$bestTune$n.trees

gbmFit$bestTune$interaction.depth

```

The best n.trees and interaction.depth parameters for this model are `r gbmFit$bestTune$n.trees` and `r gbmFit$bestTune$interaction.depth`, respectively. These values can be seen in the table below, which summarizes all the metrics for the 5-fold cross-validation. It is easy to see that these values minimize the RMSE.

```{r boostingResults}

gbm_out <- data.frame(gbmFit$results)

gbm_out <- gbm_out %>%
  arrange(RMSE)

kable(gbm_out, caption = "Output Training Metrics for Boosting",
      digits = 3, row.names = FALSE)

```

The plot below shows the RMSE by Number of Boosting Iterations and display Max Tree Depth lines for the 5-fold CV performed. It is easy to see that RMSE is minimized when n.trees = `r gbmFit$bestTune$n.trees` and interaction.depth = `r gbmFit$bestTune$interaction.depth`.

```{r boostingPlot}

plot(gbmFit)

```

The validation step for Boosting is applied on the test set after predicting the response variable for unseen data (test set). By using `predict()` and `postResample()` functions, the metrics RMSE (Root Means Squared Error), $R^2$ (Coefficient of Determination), and MAE (Mean Absolute Error) are calculated and displayed below.

```{r boostingTest}

gbm_pred <- predict(gbmFit, newdata = activeTest)

metric_boosting = postResample(gbm_pred, activeTest$shares)

metric_boosting

```

## Model Comparison & Conclusion

For the overall comparison among all 4 created models in previous sections, the test set was used for predictions and some quality of fit metrics were calculated based on these prediction on unseen data. The code below shows the function that returns the name of the best model based on RMSE values estimated on the test set. The code below displays the table comparing all 4 models.

```{r comparison}

bestMethod = function(x){
  
  bestm = which.min(lapply(1:length(x), function(i) x[[i]][1]))
  
  out = switch(bestm,
                "Random Forest",
                "Boosting",
               "LASSO Regression",
               "Linear Regression")
  
  return(out)
  
}

tb = data.frame(RF = metric_rf, Boosting = metric_boosting,
                LASSO = metric_LASSO, Linear = metric_lm)

kable(tb, caption = "Accuracy Metric by Ensemble Method on Test Set",
      digits = 3)

```


After comparing all the 4 models fit throughout this analysis, the best model was chosen based on the RMSE value, such that the model with minimum RMSE is the "winner". Therefore, the best model for the `r params$channel` data channel is **`r bestMethod(tb)`** based on RMSE metric. The RMSE, $R^2$ (coefficient of Determination), and MAE metrics for all 4 models can be seen in the table above.


